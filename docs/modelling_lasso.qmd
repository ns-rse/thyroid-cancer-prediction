### Logistic Regression

Logistic regression, even in its most basic form is, still considered a "machine learning" algorithm. However because we
do not know which out of an array of variable will be useful predictors and, following [Occam's
Razor](https://en.wikipedia.org/wiki/Occam's_razor) we would tend to prefer simpler explanatory models over complex ones
we need a method of determining what subset of variables gives good prediction. An old approach to this was to use
[Stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression), perhaps based on univariable analyses to
select which to use as a base but these approaches have fallen out favour for various reasons as they are ultimate
biased (for an overview see [@Steyerberg2001Feb]).

A popular alternative is the [Least Absolute Shrinkage and Selection Operator
(LASSO)](https://en.wikipedia.org/wiki/Lasso_(statistics)) proposed by [@Tibshirani1996] which performs [L1
regularisation](https://en.wikipedia.org/wiki/Regularized_least_squares) and allows the coefficients for variables in a
series of fitted models to "shrink" towards zero but not drop out completely. It is similar to Ridge Regression which
avoids over-fitting by reducing the sum of squares of the regression coefficients but unlike Ridge Regression it allows
variables to be selected as the coefficients can (almost) drop out by virtue of their coefficients shrinking towards
zero.

Another popular alternative is [Ridge Regression](https://en.wikipedia.org/wiki/Ridge_regression) which uses L2
regularisation and is useful when there are predictor variables that are co-linear (i.e. correlated).

It is possible to combine L1 and L2 regularisation in what is known as [Elastic
Net](https://en.wikipedia.org/wiki/Elastic_net_regularization) which combines both L1 and L2 regularisation in a linear
manner.


#### Specify the Model

Whilst we have defined the relationship between variables in the Worfklow above we now need to say what model we wish to
use to test the relationship between variables.

We first set up a simple logistic regression model using [parsnip](https://parsnip.tidymodels.org/). The `mixture`
argument is a value `0 <= mixture <= 1` which determines how much L1 regularisation is used in the model. A value of
`mixture = 1` is equivalent to full L1 regularisation and a LASSO model whilst a value of `mixture = 0` is equivalent to
full L2 regularisation and ridge regression. Here we use a `mixture = 1` which is a full LASSO model and we may wish to
inspect the results of this model and only use those variables which are of some importance in subsequent modelling
steps.

```{r}
#| label: lasso-specification
#| purl: true
#| eval: true
#| echo: true
#| output: false
lasso_tune_spec <- parsnip::logistic_reg(penalty = hardhat::tune(), mixture = 1) |>
  parsnip::set_engine("glmnet")
```

#### Tuning

The next step is to tune the model using the [`tune::tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html)
function which will calculate accuracy or Root Mean Square Error (or other metrics) for a recipe with multiple
samples. We defined our resamples above in two forms (not that we defined cross-validation as a way of sampling from our
training data above).

Re-sampling is performed at this stage by specifying the `cv_folds` to the `resamples` option. This gives more robust
estimates of the model parameters.

```{r}
#| label: lasso-tune
#| purl: true
#| eval: true
#| echo: true
#| output: false
lasso_grid <- tune::tune_grid(
  object = workflows::add_model(thyroid_workflow, lasso_tune_spec),
  resamples = cv_folds,
  grid = dials::grid_regular(penalty(), levels = 50)
)
```

#### Fit Final Model

Now select the model with the highest Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the tuned
grid search results.

```{r}
#| label: lasso-best
#| purl: true
#| eval: true
#| echo: false
#| output: false
#| cache: false
lasso_highest_roc_auc <- lasso_grid |>
  tune::select_best(metric = "roc_auc")
```

...and finalize the workflow by adding this value.

```{r}
#| label: lasso-fit-final
#| purl: true
#| eval: true
#| echo: false
#| output: false
final_lasso <- tune::finalize_workflow(
  workflows::add_model(thyroid_workflow, lasso_tune_spec),
  lasso_highest_roc_auc
)
```

#### Model Evaluation

We could collect metrics and print them here, but since we are running multiple models this step is deferred to the end.

```{r}
#| label: lasso-eval-metrics
#| purl: true
#| eval: false
#| echo: true
#| output: true
tune::last_fit(object = final_lasso, split = split) |>
  tune::collect_metrics(metrics)
```

And plot the importance of variables, note these are _not_ the same as regression coefficients, the importance values
are relative to each other, although the sign indicates whether that particular variable/level increases or decreases
risk.

```{r}
#| label: lasso-eval-importance
#| purl: true
#| eval: true
#| echo: true
#| output: true
final_lasso |>
  fit(train) |>
  hardhat::extract_fit_parsnip() |>
  vip::vi(lambda = lasso_highest_roc_auc$penalty) |>
  dplyr::mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) |>
  ggplot(mapping = aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
dark_theme_minimal()
```

``` {r}
#| label: lasso-save
#| purl: true
#| eval: true
#| echo: true
#| output: false
save(lasso_tune_spec, lasso_grid, final_lasso, lasso_highest_roc_auc, file = "data/r/lasso.RData")
```
