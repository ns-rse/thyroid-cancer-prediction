---
title: "Modelling"
bibliography: references.bib
---

```{r}
#| label: tidymodel-libraries
#| purl: true
#| eval: true
#| echo: false
#| output: false
library(dials)
library(ggdark)
library(kernlab)
library(knitr)
library(tidyverse)
library(tidymodels)
library(vip)
tidymodels_prefer()

# Set the metrics we wish to summarise
metrics <- c("roc_auc", "ppv", "npv")
```

{{< include modelling_tidymodel.qmd >}}

## Methods to Consider

There are a wealth of options when it comes to "Machine Learning", these days even logistic regression is grouped into
the term! There are however a large number of more sophisticated methods for analysing the data, and it is often wise to
apply a number of methods to ensure they are converging on similar solutions rather than cherry-picking any one method.

{{< include modelling_lasso.qmd >}}
{{< include modelling_random_forest.qmd >}}
{{< include modelling_xgboost.qmd >}}
{{< include modelling_elastic_net.qmd >}}
{{< include modelling_svm.qmd >}}



## Model Assessment

When considering the utility of prediction there are a number of metrics that can be used to determine how useful a
model is. These focus on a number of terms which have very specific statistical meaning such as [Sensitivity and
Specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).

|                 | Test Positive       | Test Negative       |
|-----------------|---------------------|---------------------|
| Actual Positive | True Positive (TP)  | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TN)  |

There are a wealth of metrics based on combinations of these True/False Positive/Negative including precision, recall
and so forth (refer to the above linked article for further details). The Tidymodels package for summarising model
performance is [yardstick](https://yardstick.tidymodels.org/) and we will use that to summarise the different metrics of
model performance and how predictive these are.

Confusion matrices such as the above can be easily generated and even plotted using `yardstick::confg_mat()` function.


{{< include modelling_predictions.qmd >}}
{{< include modelling_roc.qmd >}}
{{< include modelling_importance.qmd >}}




## TODO

I still have some things I need to add to the above workflow.

### Modelling

+ [ ] Predictive accuracy on the training data set and comparing to the test cohorts (if the predictive accuracy is worse in
      the later then the models are being over-fitted).
+ [x] Calculate ROC AUC for each model and add to legend of ROC curve plot (see
      [`yardstick::roc_auc()`](https://yardstick.tidymodels.org/reference/roc_auc.html)).
+ [ ] Add note that in the Sheffield data set the results of the LASSO may be used as a method of narrowing down which
      variables to use in subsequent models.
+ [x] Remove variables that have zero variance in the recipe creation stage. (`step_zv(all_predictors())`)
+ [ ] Add in Elastic Net regression as well as LASSO.
+ [ ] Increase the `tune::metric_set()` on which models are tuned perhaps? Most are `roc_auc` but can use `ppv`,
      `precision`, `sens` `spec`, `recall`, `accuracy` and others (see options under
      [here](https://yardstick.tidymodels.org/reference/accuracy.html#see-also)).
+ [x] Remove the predictive accuracy from each subsection (at least in the output).
+ [x] Tune the XGBoost parameters, currently have the following errors

```
 > roc_auc_xgboost <- tune::last_fit(final_xgboost, split) |>
  tune::collect_predictions() |>
  yardstick::roc_auc(final_pathology, .pred_Malignant)
+ + Error:
! 4 arguments have been tagged for tuning in these components: model_spec.
Please use one of the tuning functions (e.g. `tune_grid()`) to optimize them.
Run `rlang::last_trace()` to see where the error occurred.
```

### Quarto

+ [x] Enable cahcing so that rendering of output doesn't require fitting of models each time.
+ [ ] Sort column headings for the confusion matrices using [`#| tbl-subcap:
      []`](https://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/posts/01-side-by-side-tables/) .
+ [x] For some reason when I attempted moving this section to its own child document the `final_lasso_kfold` object
      could not be found. Work around was to save the results of each model and the load them into the
      `modelling_{predict|roc|vip}.qmd` sections.


## Notes

If you want to produce a plain R script of the code chunks in this document (or any Quarto document for that matter) you
can do so using [`knirt::purl()`](https://bookdown.org/yihui/rmarkdown-cookbook/purl.html) function and sections which
have `#| purl: true` will be included (all sections in this page have that added, none of the other Quarto documents
have that as of writing). The following will take as input `docs/modelling.qmd` and make an R script of just the code
cells in `r/modelling.R`.

``` {r}
#| echo: true
#| eval: false
knitr::purl("docs/modelling.qmd", output = "r/modelling.R")
```

## References

::: {#refs}
:::
