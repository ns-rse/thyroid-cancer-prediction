[
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "Reproducibility",
    "section": "",
    "text": "It is good practice to conduct research in an open and reproducible manner, it improves trust in the research output and allows constructive feedback on the approaches taken.\n\n\nOpen scientific research means being transparent about all steps taken in the analysis of data, from reading the raw data in, tidying it (renaming variables and deriving variables) through to running models, estimating parameters and making predictions.\nIdeally work should also aim to adhere to the FAIR Principles and where software is developed, as is the case here where code for analysis is being produced, the FAIR4RS Principles too.\nFAIR means research output is Findable, Accessible, Interoperable and Reusable. To help with this all work should on completion be added to the University of Sheffield Online Research Data Archive (ORDA).\n\n\n\nReproducibility aids transparency and openness of research. By using scripted analysis and embedding code within literate documents we can streamline the work process and provide an accurate record of how we reached our conclusions from the data provided and the methods used. Further by version controlling the development of the scripts throughout their life-cycle we have a record of how the code has changed over time.\nTo aid with this the free open-source software R will be used to undertake data cleaning/preparation, statistical modelling and report writing. It integrates with the open-source scientific publishing system Quarto and as all scripts are based on text files they can be version controlled using Git. We will work collaboratively using the GitHub forge.\nBecause R packages evolve over time as bugs are fixed and new features introduced we will use renv to define a consistent set of packages that are used for analyses.\nGetting setup with R, RStudio and Git/GitHub can be done following the excellent guide Let’s Git started | Happy Git and GitHub for the useR by Jenny Bryan.\n\n\n\nIncluded in this repository is the file references.bib which is a BibTex formatted ASCII text file of citations related to this work. BibTex works well with Quarto and make including citations in work straight-forward (see documentation)."
  },
  {
    "objectID": "reproducibility.html#open-and-reproducible-work",
    "href": "reproducibility.html#open-and-reproducible-work",
    "title": "Reproducibility",
    "section": "",
    "text": "It is good practice to conduct research in an open and reproducible manner, it improves trust in the research output and allows constructive feedback on the approaches taken.\n\n\nOpen scientific research means being transparent about all steps taken in the analysis of data, from reading the raw data in, tidying it (renaming variables and deriving variables) through to running models, estimating parameters and making predictions.\nIdeally work should also aim to adhere to the FAIR Principles and where software is developed, as is the case here where code for analysis is being produced, the FAIR4RS Principles too.\nFAIR means research output is Findable, Accessible, Interoperable and Reusable. To help with this all work should on completion be added to the University of Sheffield Online Research Data Archive (ORDA).\n\n\n\nReproducibility aids transparency and openness of research. By using scripted analysis and embedding code within literate documents we can streamline the work process and provide an accurate record of how we reached our conclusions from the data provided and the methods used. Further by version controlling the development of the scripts throughout their life-cycle we have a record of how the code has changed over time.\nTo aid with this the free open-source software R will be used to undertake data cleaning/preparation, statistical modelling and report writing. It integrates with the open-source scientific publishing system Quarto and as all scripts are based on text files they can be version controlled using Git. We will work collaboratively using the GitHub forge.\nBecause R packages evolve over time as bugs are fixed and new features introduced we will use renv to define a consistent set of packages that are used for analyses.\nGetting setup with R, RStudio and Git/GitHub can be done following the excellent guide Let’s Git started | Happy Git and GitHub for the useR by Jenny Bryan.\n\n\n\nIncluded in this repository is the file references.bib which is a BibTex formatted ASCII text file of citations related to this work. BibTex works well with Quarto and make including citations in work straight-forward (see documentation)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thyroid Cancer Prediction",
    "section": "",
    "text": "This project details work on the prediction of thyroid cancer.\n\nLiterature\nData Description\nModelling\nCitations\nLinks\n\nThe site is written in Quarto and published via GitHub Pages."
  },
  {
    "objectID": "citations.html",
    "href": "citations.html",
    "title": "Citations",
    "section": "",
    "text": "For BibTeX references see references.bib\n\nCohort Analysis of Clinical and Ultrasound Variables Predicting Cancer Risk in 20,001 Consecutive Thyroid Nodules\nImproving the diagnosis of thyroid cancer by machine learning and clinical data | Scientific Reports\nMcGill Thyroid Nodule Score (MTNS): \"rating the risk,\" a novel predictive scheme for cancer risk determination. - Abstract - Europe PMC\nNomogram for predicting malignancy in thyroid nodules using clinical, biochemical, ultrasonographic, and cytologic features - ScienceDirect\nThe TNAPP web-based algorithm improves thyroid nodule management in clinical practice: A retrospective validation study\nThe FAIR Guiding Principles for scientific data management and stewardship | Scientific Data"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Description",
    "section": "",
    "text": "Details of the data set."
  },
  {
    "objectID": "data.html#source",
    "href": "data.html#source",
    "title": "Data Description",
    "section": "Source",
    "text": "Source\nThe data is derived from specialist Thyroid cancer units where patients presenting with symptoms (e.g. nodules) are assessed in greater detail and decisions are made as to whether imaging or further biopsies are required."
  },
  {
    "objectID": "data.html#data",
    "href": "data.html#data",
    "title": "Data Description",
    "section": "Data",
    "text": "Data\nAn ASCII CSV file (Thy3000_DATA_LABELS_Raw.csv) with data on 584 cases has been provided. There is no data dictionary defining what each field is nor its type available yet.\nNB Some of the column headers have commas (,) in the variable names, this can cause problems when reading CSV files. Such strings should be double-quoted which appears to be the case but something to check carefully.\n\n\n\n\n\n\n\n\n\nVariable (as received)\nType\nDescription\nRenamed to…\n\n\n\n\nRecord ID\nstr\nUnique identifier.\nrecord_id\n\n\nData Access Group\nstr\nCenter\ncenter\n\n\nStudy ID\nstr\nStudy Identifiers.\nstudy_id\n\n\n1.1 Date of referral\ndate\nDate of referral\nreferral_date\n\n\n1.2 Which clinic was the patient recruited from?\nstr\nRecruting clinic.\nrecruiting_clinic\n\n\nIf Other\nstr\nOther referring clinic.\n\n\n\n1.3. The date the patient was seen in clinic\ndate\nDate patient was seen in clinic.\nclinc_date\n\n\n1.4 Referral source\nstr\nSource of referral\nreferral_source\n\n\n\"If Other please specify\nstr\nOther referring source\n\n\n\n\"1.4.1 If GP was it 2-week wait referral?\"\n\n\n\n\n\n1.5. Presentation\n\n\n\n\n\nComplete?\n\n\n\n\n\n2.1. Age of the patient when seen in clinic\n\n\n\n\n\n2.2. Body Mass Index of patient\n\n\n\n\n\n2.3. Smoking status\n\n\n\n\n\n2.4. Previous neck irradiation\n\n\n\n\n\n2.5 American Society of Anaesthesiologist (ASA) score\n\n\n\n\n\nComplete?\n\n\n\n\n\n3.1. Presentation (choice=Neck symptoms)\n\n\n\n\n\n3.1. Presentation (choice=Incidental lesion on imaging)\n\n\n\n\n\n3.1. Presentation (choice=HypErthyroidism on thyroid function test)\n\n\n\n\n\n3.1. Presentation (choice=HypOthyroidism on thyroid function test)\n\n\n\n\n\n3.1. Presentation (choice=Symptoms of abnormal thyroid function)\n\n\n\n\n\n3.1. Presentation (choice=Not known)\n\n\n\n\n\n3.1. Presentation (choice=Other)\n\n\n\n\n\nIf other please specify\"\n\n\n\n\n\n3.1.1 Symptomatology (choice=No symptoms)\n\n\n\n\n\n\"3.1.1 Symptomatology (choice=Neck lump (noted by patient family or doctor))\"\n\n\n\n\n\n3.1.1 Symptomatology (choice=Compressive symptoms (breathing or swallowing difficulty or voice change))\n\n\n\n\n\n3.1.1 Symptomatology (choice=Symptoms of thyroid dysfunction)\n\n\n\n\n\n3.1.1 Symptomatology (choice=Other)\n\n\n\n\n\n\"If other please specify\"\n\n\n\n\n\n3.1.2 Was this nodule found incidentally on imaging?\n\n\n\n\n\n\"If yes what imaging\"\n\n\n\n\n\n3.2 Clinical Assessment\n\n\n\n\n\n3.2.1 Retrosternal on clinical examination\n\n\n\n\n\n3.2.2 Palpable lymphadenopathy\n\n\n\n\n\n3.2.3 Patient perception of rapid growth of nodule\n\n\n\n\n\n3.4 Thyroid function tests done within 3 months of presentation to clinic\n\n\n\n\n\n3.5 Ultrasound performed\n\n\n\n\n\n3.5.1 Reported maximum diameter of the largest thyroid nodule on ultrasound in millimetres\n\n\n\n\n\n3.5.2 Description of thyroid nodule(s) on ultrasound\n\n\n\n\n\n\"3.5.3 If Ultrasound performed\n\n\n\n\n\nU stage reported: \"\n\n\n\n\n\n\"3.5.4 If Ultrasound performed\n\n\n\n\n\nTIRADS reported \"\n\n\n\n\n\n\"3.5.5 If Ultrasound performed\n\n\n\n\n\nwas lymphadenopathy documented\"\n\n\n\n\n\n3.5.6 Elastography performed\n\n\n\n\n\n3.5.7 CT neck performed\n\n\n\n\n\n3.5.8 If CT neck performed reason for performing it (choice=Extrathyroid extention/local invasion)\n\n\n\n\n\n3.5.8 If CT neck performed reason for performing it (choice=Retrosternal extension/plan surgical approach)\n\n\n\n\n\n3.5.8 If CT neck performed reason for performing it (choice=Extent of lymphadenopathy)\n\n\n\n\n\n3.5.8 If CT neck performed reason for performing it (choice=Tracheal compression)\n\n\n\n\n\n3.5.8 If CT neck performed reason for performing it (choice=Not known)\n\n\n\n\n\n3.5.8 If CT neck performed reason for performing it (choice=Unrelated to thyroid pathology)\n\n\n\n\n\n3.5.9 MRI neck performed\n\n\n\n\n\n3.5.10 If MRI neck performed reason for performing it (choice=Extrathyroid extention/local invasion)\n\n\n\n\n\n3.5.10 If MRI neck performed reason for performing it (choice=Retrosternal extension/plan surgical approach)\n\n\n\n\n\n3.5.10 If MRI neck performed reason for performing it (choice=Extent of lymphadenopathy)\n\n\n\n\n\n3.5.10 If MRI neck performed reason for performing it (choice=Tracheal compression)\n\n\n\n\n\n3.5.10 If MRI neck performed reason for performing it (choice=Not known)\n\n\n\n\n\n3.5.10 If MRI neck performed reason for performing it (choice=Unrelated to thyroid pathology)\n\n\n\n\n\n3.5.11 Iodine-123 scan performed\n\n\n\n\n\n3.6. FNA of thyroid nodule performed (either at time of ultrasound or later)\n\n\n\n\n\n3.6.1 If FNA performed was Thy or Bethesda stage reported\"\n\n\n\n\n\n3.6.1.1 Thy result\n\n\n\n\n\n3.6.1.2 Bethesda result\n\n\n\n\n\n3.7 Core biopsy performed\n\n\n\n\n\n3.8 FNA of lymph node performed\n\n\n\n\n\n3.8.1 If FNA lymph node performed result\n\n\n\n\n\nComplete?\n\n\n\n\n\n4.1 Initial management decision\n\n\n\n\n\n4.2 Date of decision (either clinic letter or MDT date)\n\n\n\n\n\n4.3 Date of surgery or start of other treatment (if interventional)\n\n\n\n\n\n4.4. In case of no intervention was a routine review offered\n\n\n\n\n\n4.4.1 If yes what was the planned interval (in weeks)\n\n\n\n\n\n4.4.2 Was Ultrasound repeated at routine review?\n\n\n\n\n\n4.4.3 Was FNA repeated at routine review\n\n\n\n\n\n4.4.4 If routine review performed was management strategy changed at review\n\n\n\n\n\n4.4.4.1 If management strategy changed at review revised management decision\n\n\n\n\n\n\"4.4.4.2 If yes reason for change in management\"\n\n\n\n\n\nIf other\n\n\n\n\n\n4.4.4.3 If no change in management decision: date last seen by 'thyroid' team\n\n\n\n\n\n\"4.4.4.4 If no change in management strategy at review\n\n\n\n\n\nconfirm management plan when last seen\"\n\n\n\n\n\n4.4.5 Patient was signposted to appropriate patient SUPPORT organisation and/or provided with written PATIENT information about thyroid nodules (including leaflets)\n\n\n\n\n\nComplete?\n\n\n\n\n\n5.1 Type of thyroid surgery\n\n\n\n\n\n5.1.1 Lymph node dissection done at this surgery\n\n\n\n\n\n5.1.2 Pathology\n\n\n\n\n\n\"If other cancer/other diagnosis please state\"\n\n\n\n\n\nComplete?\n\n\n\n\n\n\nTabular and graphical summaries of the data provided in Thy3000_DATA_LABELS_Raw.csv after cleaning we can summarise the data. In total there were 541 recorded from a total of 13."
  },
  {
    "objectID": "data.html#quantitative-variables",
    "href": "data.html#quantitative-variables",
    "title": "Data Description",
    "section": "Quantitative Variables",
    "text": "Quantitative Variables\n\nStatistics\n\n\n\nSummary statistics for quantitative variables\n\n\nVariable\nMean\nSD\nLower IQR\nMedian\nUpper IQR\nMin\nMax\n\n\n\n\nage\n54.518\n16.683\n42\n54.00\n68.000\n12.0\n91\n\n\nbmi\n27.760\n5.720\n24\n27.05\n30.925\n17.1\n50\n\n\n\n\n\n\n\nAge\n\n\n\n\n\nDistribution of Ages at referral\n\n\n\n\n\n\nBody Mass Index (BMI)\n\n\n\n\n\nDistribution of Body Mass Index (BMI)\n\n\n\n\n\n\nNodule Maximum Diameter\n\n\n\n\n\nDistribution of Nodule Maximum Diameter (mm)"
  },
  {
    "objectID": "data.html#categorical-variables",
    "href": "data.html#categorical-variables",
    "title": "Data Description",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nASA Score\n\n\n\n\n\nReported ASA score\n\n\n\n\n\n\n\nReported ASA score\n\n\nasa_score\nn\n%\n\n\n\n\nI\n186\n32.292\n\n\nII\n201\n34.896\n\n\nIII\n70\n12.153\n\n\nIV\n3\n0.521\n\n\nNA\n116\n20.139\n\n\n\n\n\n\n\nData Access Group\n\n\n\n\n\nData Access Group\n\n\n\n\n\n\n\nData Access Group\n\n\ndata_access_group\nn\n%\n\n\n\n\nBarnsley\n35\n6.076\n\n\nCardiff and Vale\n44\n7.639\n\n\nDoncaster\n19\n3.299\n\n\nHull\n31\n5.382\n\n\nNHS Tayside\n30\n5.208\n\n\nNorfolk & Norwich\n30\n5.208\n\n\nPortsmouth\n32\n5.556\n\n\nRoyal Berkshire\n106\n18.403\n\n\nSalford\n40\n6.944\n\n\nSheffield Teaching Hospitals Foundation Trust\n116\n20.139\n\n\nUniversity Hospitals of North Midlands\n56\n9.722\n\n\nWye Valley Trust\n33\n5.729\n\n\nNA\n4\n0.694\n\n\n\n\n\n\n\nReferral Source\n\n\n\n\n\nReferral Source\n\n\n\n\n\n\n\nReferral Source\n\n\nreferral_source\nn\n%\n\n\n\n\nGP\n423\n73.438\n\n\nsecondary care\n149\n25.868\n\n\nNA\n4\n0.694\n\n\n\n\n\n\n\nRecruiting Clinic\n\n\n\n\n\nRecruiting Clinic\n\n\n\n\n\n\n\nRecruiting Clinic\n\n\nclinic_recruiting\nn\n%\n\n\n\n\nENT\n277\n48.090\n\n\nGP\n25\n4.340\n\n\nGeneral surgery (including endocrine surgery)\n127\n22.049\n\n\nJoint thyroid\n75\n13.021\n\n\nMedicine\n58\n10.069\n\n\nOther\n11\n1.910\n\n\nNA\n3\n0.521\n\n\n\n\n\n\n\nThyroid Nodule FNA\n\n\n\n\n\nThyroid Nodule FNA\n\n\n\n\n\n\n\nThyroid Nodule FNA\n\n\nnodule_fna_thy\nn\n%\n\n\n\n\nThy1\n66\n11.458\n\n\nThy2\n113\n19.618\n\n\nThy3\n94\n16.319\n\n\nThy4\n14\n2.431\n\n\nThy5\n14\n2.431\n\n\nNA\n275\n47.743\n\n\n\n\n\n\n\nFinal Pathology\n\n\n\n\n\nFinal Pathology\n\n\n\n\n\n\n\nFinal Pathology\n\n\nfinal_pathology\nn\n%\n\n\n\n\nBenign\n169\n29.340\n\n\nMalignant\n75\n13.021\n\n\nNA\n332\n57.639\n\n\n\n\n\n\n\nFinal Pathology x Thyroid Nodule FNA\nIt is useful to check that the rules we have derived for classifying Final Pathology, which are based on thyroid_surgery_lymph_node_pathology when available and nodule_fna_thy when not are correct. We can do this by tabulating the data (a heatmap could be plotted that graphically shows the data distribution using the geom_tile() geometry).\n\n\n\nSurgical Lymph Node Pathology and Thyroid Nodule FNA. Percentages are by Pathology\n\n\n\n\n\n\n\n\n\n\n\nThyroid Surgery Lymph Node Pathology\nThy1 (%)\nThy2 (%)\nThy3 (%)\nThy4 (%)\nThy5 (%)\nMissing (%)\n\n\n\n\nAnaplastic cancer\nNA\nNA\nNA\nNA\n1 (100.000)\nNA\n\n\nAuto immune thyroiditis\n1 (25.000)\nNA\n1 (25.000)\nNA\nNA\n2 (50.000)\n\n\nColloid adenoma\n1 (16.667)\n1 (16.667)\n2 (33.333)\n1 (16.667)\nNA\n1 (16.667)\n\n\nColloid goitre\n7 (20.000)\n6 (17.143)\n7 (20.000)\nNA\nNA\n15 (42.857)\n\n\nFollicular adenoma\n3 (13.043)\n1 ( 4.348)\n18 (78.261)\n1 ( 4.348)\nNA\nNA\n\n\nFollicular thyroid cancer\n1 ( 7.692)\n1 ( 7.692)\n10 (76.923)\nNA\nNA\n1 ( 7.692)\n\n\nGraves’ disease\n1 (100.000)\nNA\nNA\nNA\nNA\nNA\n\n\nHürthle cell/oncocytic adenoma\n1 (10.000)\n2 (20.000)\n6 (60.000)\n1 (10.000)\nNA\nNA\n\n\nHürthle cell/oncocytic carcinoma\n1 (20.000)\nNA\n3 (60.000)\nNA\n1 (20.000)\nNA\n\n\nMedullary thyroid cancer\nNA\nNA\nNA\n1 (50.000)\nNA\n1 (50.000)\n\n\nOther cancer / Other diagnosis\n2 ( 6.452)\n11 (35.484)\n9 (29.032)\nNA\nNA\n9 (29.032)\n\n\nPapillary thyroid cancer\n5 (10.204)\n4 ( 8.163)\n15 (30.612)\n8 (16.327)\n12 (24.490)\n5 (10.204)\n\n\nSimple cyst\n2 (33.333)\nNA\nNA\nNA\nNA\n4 (66.667)\n\n\nNA\n41 (10.513)\n87 (22.308)\n23 ( 5.897)\n2 ( 0.513)\nNA\n237 (60.769)\n\n\n\n\n\nWe may be interested in the percentages across all observations though.\n\n\n\nSurgical Lymph Node Pathology and Thyroid Nodule FNA. Percentages are across all observations.\n\n\n\n\n\n\n\n\n\n\n\nThyroid Surgery Lymph Node Pathology\nThy1 (%)\nThy2 (%)\nThy3 (%)\nThy4 (%)\nThy5 (%)\nMissing (%)\n\n\n\n\nAnaplastic cancer\nNA\nNA\nNA\nNA\n1 ( 0.174)\nNA\n\n\nAuto immune thyroiditis\n1 ( 0.174)\nNA\n1 ( 0.174)\nNA\nNA\n2 ( 0.347)\n\n\nColloid adenoma\n1 ( 0.174)\n1 ( 0.174)\n2 ( 0.347)\n1 ( 0.174)\nNA\n1 ( 0.174)\n\n\nColloid goitre\n7 ( 1.215)\n6 ( 1.042)\n7 ( 1.215)\nNA\nNA\n15 ( 2.604)\n\n\nFollicular adenoma\n3 ( 0.521)\n1 ( 0.174)\n18 ( 3.125)\n1 ( 0.174)\nNA\nNA\n\n\nFollicular thyroid cancer\n1 ( 0.174)\n1 ( 0.174)\n10 ( 1.736)\nNA\nNA\n1 ( 0.174)\n\n\nGraves’ disease\n1 ( 0.174)\nNA\nNA\nNA\nNA\nNA\n\n\nHürthle cell/oncocytic adenoma\n1 ( 0.174)\n2 ( 0.347)\n6 ( 1.042)\n1 ( 0.174)\nNA\nNA\n\n\nHürthle cell/oncocytic carcinoma\n1 ( 0.174)\nNA\n3 ( 0.521)\nNA\n1 ( 0.174)\nNA\n\n\nMedullary thyroid cancer\nNA\nNA\nNA\n1 ( 0.174)\nNA\n1 ( 0.174)\n\n\nOther cancer / Other diagnosis\n2 ( 0.347)\n11 ( 1.910)\n9 ( 1.562)\nNA\nNA\n9 ( 1.562)\n\n\nPapillary thyroid cancer\n5 ( 0.868)\n4 ( 0.694)\n15 ( 2.604)\n8 ( 1.389)\n12 ( 2.083)\n5 ( 0.868)\n\n\nSimple cyst\n2 ( 0.347)\nNA\nNA\nNA\nNA\n4 ( 0.694)\n\n\nNA\n41 ( 7.118)\n87 (15.104)\n23 ( 3.993)\n2 ( 0.347)\nNA\n237 (41.146)\n\n\n\n\n\nTabulating the Thyroid Nodule FNA against the final prediction\n\n\n\nFinal Pathology and Thyroid Nodule FNA. Percentages are across all observations.\n\n\nNodule FNA\nBenign (%)\nMalignant (%)\nMissing (%)\n\n\n\n\nThy1\n15 ( 2.604)\n8 ( 1.389)\n43 ( 7.465)\n\n\nThy2\n97 (16.840)\n5 ( 0.868)\n11 ( 1.910)\n\n\nThy3\n33 ( 5.729)\n31 ( 5.382)\n30 ( 5.208)\n\n\nThy4\n3 ( 0.521)\n9 ( 1.562)\n2 ( 0.347)\n\n\nThy5\nNA\n14 ( 2.431)\nNA\n\n\nNA\n21 ( 3.646)\n8 ( 1.389)\n246 (42.708)\n\n\n\n\n\nWe can make a stacked Bar chart of this, but first we filter out instances where the final pathology is missing (i.e. NA)"
  },
  {
    "objectID": "modelling.html",
    "href": "modelling.html",
    "title": "Modelling",
    "section": "",
    "text": "The R packages included in the Tidymodels provide an excellent framework for undertaking the modelling aspect of the work.\nA very useful training workshop was held 2023-10-18 as part of the R in Pharma event. Nicola Rennie has made her material is available on-line and it provides a good starting point for applying the various modelling methodologies to prediction of Thyroid cancer.\n\nSlides\nGitHub Repo\n\nWhilst this provides an excellent introduction to the Tidy Modelling framework the book Tidy Modeling with R goes deeper into the methods and options available. Another useful reference on which this books builds is R for Data Science (2e) which should serve as a useful reference for learning R and adopting good practices.\nIt is also recommended to read the documentation that goes with the Tidymodels package, in particular the Get Started page which includes a predictive modelling case study.\n\n\nIn the absence of the data set that is to be analysed we simulate some dummy data on which to demonstrate the methods.\n\n\n\nTo get going with Tidymodels we first need to split our data into Testing and Training subsets. This is done so that we do not have an over-fitted model as we fit the model to the training subset and then test its predictive accuracy in the subset that we withheld, the test subset.\nThe allocation of individuals to train or test is performed randomly and so we set a seed to ensure the pseudo-random number generator produces the same split each and every time this script is run (this makes our work reproducible). A decision has to be made about how to split the data, often a slightly larger proportion is used for the training subset, here we chose to use a 3:1 split (i.e. 75% of observations are used in the training set).\n\nset.seed(5039378)\nsplit &lt;- rsample::initial_split(dummy, prop = 0.75)\ntrain &lt;- rsample::training(split)\ntest &lt;- rsample::testing(split)\n\n\n\n\nWe will still want to make some assessment of the model estimated from the training set and this is achieved by resampling which involves taking subsets of our training data and fitting the models on those subsets and then looking at the distribution of metrics across the multiple model fits. A commonly used approach for this methodology is Cross-Validation. V-fold cross-validation splits the data into V folds, the first fold is excluded from the data set and the model assessed, then this subset is replaced and the second fold is excluded and the model assessed again. This is repeated until each fold has been excluded and the model estimated on the remainder. This method can then be repeated R times where the folds are varied in each repetition to give a better estimate of the model parameters.\nThe following figure gives an overview of how V-fold cross-validation works (without repetition).\n\n\n\nV-fold cross-validation of training data. Source: Feature Engineering and Selection: A Practical Approach for Predictive Models\n\n\nWe can set this up by passing the train subset into the rsample::vflod_cv() function and define the number of folds (v) and the number of repeats (repeats).\n\ncv_folds &lt;- rsample::vfold_cv(train, v = 10, repeats = 10)\n\nAnother method of cross-validation is Leave One Out where one observation is removed from the (training) data set, the model is fitted on the remaining samples and then used to make a prediction on the excluded sample. This is then repeated on all samples. We can set this up using the rsample::loo_cv() function.\n\ncv_loo &lt;- rsample::loo_cv(train)\n\n\n\n\nIn the Tidymodels framework the starting point is to create a Recipe, this sets up the “ingredients” for the model and defines what steps should be taken prior to fitting the model, regardless of what model is being fitted. Steps that can go into making a recipe are…\n\nDefine the model in terms of outcome variable and predictors.\nCreating dummy variables for categorical variables.\nNormalizing data, e.g. log-transformation, rescaling.\n\nNote that we pass only the train data into the recipe so that models are fitted on the training data.\n\nthyroid_recipe &lt;- recipes::recipe(final_pathology ~ gender + nodule_size + age + asa_score + smoking_status + nodule_fna_thy, data = train) |&gt;\n  # recipes::step_num2factor(final_pathology, levels = c(\"Benign\", \"Malignant\")) |&gt;\n  recipes::step_dummy(gender, asa_score, smoking_status, nodule_fna_thy) |&gt;\n  recipes::step_normalize(all_numeric())\n\n\n\n\nOnce a recipe has been defined it can be added to a workflow which will apply this step every time the workflow is run using the different models and post-processing steps that we will add.\n\nthyroid_workflow &lt;- workflows::workflow() |&gt;\n  workflows::add_recipe(thyroid_recipe)"
  },
  {
    "objectID": "modelling.html#tidymodelling",
    "href": "modelling.html#tidymodelling",
    "title": "Modelling",
    "section": "",
    "text": "The R packages included in the Tidymodels provide an excellent framework for undertaking the modelling aspect of the work.\nA very useful training workshop was held 2023-10-18 as part of the R in Pharma event. Nicola Rennie has made her material is available on-line and it provides a good starting point for applying the various modelling methodologies to prediction of Thyroid cancer.\n\nSlides\nGitHub Repo\n\nWhilst this provides an excellent introduction to the Tidy Modelling framework the book Tidy Modeling with R goes deeper into the methods and options available. Another useful reference on which this books builds is R for Data Science (2e) which should serve as a useful reference for learning R and adopting good practices.\nIt is also recommended to read the documentation that goes with the Tidymodels package, in particular the Get Started page which includes a predictive modelling case study.\n\n\nIn the absence of the data set that is to be analysed we simulate some dummy data on which to demonstrate the methods.\n\n\n\nTo get going with Tidymodels we first need to split our data into Testing and Training subsets. This is done so that we do not have an over-fitted model as we fit the model to the training subset and then test its predictive accuracy in the subset that we withheld, the test subset.\nThe allocation of individuals to train or test is performed randomly and so we set a seed to ensure the pseudo-random number generator produces the same split each and every time this script is run (this makes our work reproducible). A decision has to be made about how to split the data, often a slightly larger proportion is used for the training subset, here we chose to use a 3:1 split (i.e. 75% of observations are used in the training set).\n\nset.seed(5039378)\nsplit &lt;- rsample::initial_split(dummy, prop = 0.75)\ntrain &lt;- rsample::training(split)\ntest &lt;- rsample::testing(split)\n\n\n\n\nWe will still want to make some assessment of the model estimated from the training set and this is achieved by resampling which involves taking subsets of our training data and fitting the models on those subsets and then looking at the distribution of metrics across the multiple model fits. A commonly used approach for this methodology is Cross-Validation. V-fold cross-validation splits the data into V folds, the first fold is excluded from the data set and the model assessed, then this subset is replaced and the second fold is excluded and the model assessed again. This is repeated until each fold has been excluded and the model estimated on the remainder. This method can then be repeated R times where the folds are varied in each repetition to give a better estimate of the model parameters.\nThe following figure gives an overview of how V-fold cross-validation works (without repetition).\n\n\n\nV-fold cross-validation of training data. Source: Feature Engineering and Selection: A Practical Approach for Predictive Models\n\n\nWe can set this up by passing the train subset into the rsample::vflod_cv() function and define the number of folds (v) and the number of repeats (repeats).\n\ncv_folds &lt;- rsample::vfold_cv(train, v = 10, repeats = 10)\n\nAnother method of cross-validation is Leave One Out where one observation is removed from the (training) data set, the model is fitted on the remaining samples and then used to make a prediction on the excluded sample. This is then repeated on all samples. We can set this up using the rsample::loo_cv() function.\n\ncv_loo &lt;- rsample::loo_cv(train)\n\n\n\n\nIn the Tidymodels framework the starting point is to create a Recipe, this sets up the “ingredients” for the model and defines what steps should be taken prior to fitting the model, regardless of what model is being fitted. Steps that can go into making a recipe are…\n\nDefine the model in terms of outcome variable and predictors.\nCreating dummy variables for categorical variables.\nNormalizing data, e.g. log-transformation, rescaling.\n\nNote that we pass only the train data into the recipe so that models are fitted on the training data.\n\nthyroid_recipe &lt;- recipes::recipe(final_pathology ~ gender + nodule_size + age + asa_score + smoking_status + nodule_fna_thy, data = train) |&gt;\n  # recipes::step_num2factor(final_pathology, levels = c(\"Benign\", \"Malignant\")) |&gt;\n  recipes::step_dummy(gender, asa_score, smoking_status, nodule_fna_thy) |&gt;\n  recipes::step_normalize(all_numeric())\n\n\n\n\nOnce a recipe has been defined it can be added to a workflow which will apply this step every time the workflow is run using the different models and post-processing steps that we will add.\n\nthyroid_workflow &lt;- workflows::workflow() |&gt;\n  workflows::add_recipe(thyroid_recipe)"
  },
  {
    "objectID": "modelling.html#methods-to-consider",
    "href": "modelling.html#methods-to-consider",
    "title": "Modelling",
    "section": "Methods to Consider",
    "text": "Methods to Consider\nThere are a wealth of options when it comes to “Machine Learning”, these days even logistic regression is grouped into the term! There are however a large number of more sophisticated methods for analysing the data, and it is often wise to apply a number of methods to ensure they are converging on similar solutions rather than cherry-picking any one method.\n\nLogistic Regression\nLogistic regression, even in its most basic form is, still considered a “machine learning” algorithm. However because we do not know which out of an array of variable will be useful predictors and, following Occam’s Razor we would tend to prefer simpler explanatory models over complex ones we need a method of determining what subset of variables gives good prediction. An old approach to this was to use Stepwise regression, perhaps based on univariable analyses to select which to use as a base but these approaches have fallen out favour for various reasons as they are ultimate biased (for an overview see (Steyerberg et al. 2001)).\nA popular alternative is the Least Absolute Shrinkage and Selection Operator (LASSO) proposed by (Tibshirani 1996) which performs L1 regularisation and allows the coefficients for variables in a series of fitted models to “shrink” towards zero but not drop out completely. It is similar to Ridge Regression which avoids over-fitting by reducing the sum of squares of the regression coefficients but unlike Ridge Regression it allows variables to be selected as the coefficients can (almost) drop out by virtue of their coefficients shrinking towards zero.\nAnother popular alternative is Ridge Regression which uses L2 regularisation and is useful when there are predictor variables that are co-linear (i.e. correlated).\nIt is possible to combine L1 and L2 regularisation in what is known as Elastic Net which combines both L1 and L2 regularisation in a linear manner.\n\nSpecify the Model\nWhilst we have defined the relationship between variables in the Worfklow above we now need to say what model we wish to use to test the relationship between variables.\nWe first set up a simple logistic regression model using parsnip. The mixture argument is a value 0 &lt;= mixture &lt;= 1 which determines how much L1 regularisation is used in the model. A value of mixture = 1 is equivalent to full L1 regularisation and a LASSO model whilst a value of mixture = 0 is equivalent to full L2 regularisation and ridge regression. Here we use a mixture = 1 which is a full LASSO model and we may wish to inspect the results of this model and only use those variables which are of some importance in subsequent modelling steps.\n\ntune_spec_lasso &lt;- parsnip::logistic_reg(penalty = hardhat::tune(), mixture = 1) |&gt;\n  parsnip::set_engine(\"glmnet\")\n\n\n\nTuning\nThe next step is to tune the model using the tune::tune_grid() function which will calculate accuracy or Root Mean Square Error (or other metrics) for a recipe with multiple samples. We defined our resamples above in two forms (not that we defined cross-validation as a way of sampling from our training data above).\nRe-sampling is performed at this stage by specifying the cv_folds to the resamples option. This gives more robust estimates of the model parameters.\n\nlasso_grid &lt;- tune::tune_grid(\n  object = workflows::add_model(thyroid_workflow, tune_spec_lasso),\n  resamples = cv_folds,\n  grid = dials::grid_regular(penalty(), levels = 50)\n)\n\n\n\nFit Final Model\nNow select the model with the highest Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the tuned grid search results.\n…and finalize the workflow by adding this value.\n\n\nModel Evaluation\nCollect metrics using the original data.\n\ntune::last_fit(object = final_lasso_kfold, split = split) |&gt;\n  tune::collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.688 Preprocessor1_Model1\n2 roc_auc  binary         0.455 Preprocessor1_Model1\n\n\nAnd plot the importance of variables, note these are not the same as regression coefficients, the importance values are relative to each other, although the sign indicates whether that particular variable/level increases or decreases risk.\n\nfinal_lasso_kfold |&gt;\n  fit(train) |&gt;\n  hardhat::extract_fit_parsnip() |&gt;\n  vip::vi(lambda = lasso_kfold_roc_auc$penalty) |&gt;\n  dplyr::mutate(\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance)\n  ) |&gt;\n  ggplot(mapping = aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  dark_theme_minimal()\n\nInverted geom defaults of fill and color/colour.\nTo change them back, use invert_geom_defaults().\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\nRandom forests build on the concept of regression trees by building many such trees on random subsets of data and averaging them. They build “deep” trees partitioning the data each time testing to see which variable provides the optimal split (best prediction into the Malignant/Benign categorisation). Adding splits at each recursive branch until all individuals are classified. Part of the art is deciding how “deep” to go as too many partitions results in over-fitting and lack of generalisability.\nRandom Forests itteratively search through the variables to work out which provides the best method of classifying people into either Malignant or Benign“. It tries all variables in# turn. For binary variables such as gender this is straight-forward. For continuous variables a number of different thresholds for dichotomising the variable are tested for each possible split and the optimal cut-point is selected. This process is repeated so that after for example after selecting gender to be a strong predictor the remaining variables will tested for their predictive ability and the best selected. The number of trees that are created and tested is defined by the trees parameter which here is set to 100. For binary variables such as gender this is straigh-forward but for continuous variables different thresholds for splitting are tested and the optimal point is selected.\nThis whole process is repeated a number of times building lots of trees (i.e. random forests) and it is in this regard an “ensemble” method as it is averaging across many possible methods.\n\nrf_tune &lt;- parsnip::rand_forest(\n  mtry = tune(),\n  trees = 100,\n  min_n = tune()\n) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"ranger\", importance = \"impurity\")\n\nAs before the process is repeated using Cross validation on the training set.\n\nrf_grid &lt;- tune::tune_grid(\n  add_model(thyroid_workflow, rf_tune),\n  resamples = cv_folds, ## cv_loo,\n  grid = grid_regular(mtry(range = c(5, 10)), # smaller ranges will run quicker\n    min_n(range = c(2, 25)),\n    levels = 3\n  )\n)\n\n…and the best model based on the ROC AUC selected…\n\nrf_highest_roc_auc &lt;- rf_grid |&gt;\n  select_best(\"roc_auc\")\nfinal_rf &lt;- tune::finalize_workflow(\n  add_model(thyroid_workflow, rf_tune),\n  rf_highest_roc_auc\n)\n\n…and the metrics calculated.\n\ntune::last_fit(final_rf, split) |&gt;\n  tune::collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.639 Preprocessor1_Model1\n2 roc_auc  binary         0.528 Preprocessor1_Model1\n\n\n\nfinal_rf |&gt;\n  fit(train) |&gt;\n  hardhat::extract_fit_parsnip() |&gt;\n  vip::vi(lambda = final_rf$penalty) |&gt;\n  dplyr::mutate(\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance)\n  ) |&gt;\n  ggplot(mapping = aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  dark_theme_minimal()\n\n\n\nGradient Boosting\nGradient Boosting is another “ensemble” method but in contrast to Random Forests where deep partitioning trees are formed, gradient boosting uses “shallow” trees with simpler decision rules and layers are built in a stage-wise fashion. It allows a choice of loss functions for optimisation and typically out-performs Random Forests when it comes to prediction modelling.\nA useful article showing how to use the XGBoost package with Tidymodels (via parsnip::boost_tree()) is here and informed the development of this section.\n\nxgboost_model &lt;- parsnip::boost_tree(\n  mode = \"classification\",\n  trees = 100,\n  min_n = tune(),\n  tree_depth = tune(),\n  learn_rate = tune(),\n  loss_reduction = tune()\n) |&gt;\n  set_engine(\"xgboost\", objective = \"binary:logistic\")\n\nWe use the dials package to set tuning parameters for the model along with the grid space we are to use (there are lots of options here) this helps identify the hyperparameters with the lowest prediction error.\n\nxgboost_params &lt;- dials::parameters(\n  min_n(),\n  tree_depth(),\n  learn_rate(),\n  loss_reduction()\n)\nxgboost_grid &lt;- dials::grid_max_entropy(\n  xgboost_params,\n  size = 10\n)\n\nFinally the model is tuned\n\nxgboost_tuned &lt;- tune::tune_grid(workflows::add_model(thyroid_workflow, spec = xgboost_model),\n  resamples = cv_folds,\n  grid = xgboost_grid,\n  metrics = yardstick::metric_set(roc_auc, accuracy, ppv),\n  control = tune::control_grid(verbose = FALSE)\n)\n\nWe get the best final fit from the Gradient Boosting model.\n\nxgboost_highest_roc_auc &lt;- xgboost_tuned |&gt;\n  tune::select_best(\"roc_auc\")\nfinal_xgboost &lt;- tune::finalize_workflow(\n  add_model(thyroid_workflow, xgboost_model),\n  xgboost_highest_roc_auc\n)\n\n\n\nSupport Vector Machine\nYet another modelling approach is Support Vector Machines (SVMs) and again we step through the process of specify the model using the svm_rbf() function which aims to “maximize the width of the margin between classes using a nonlinear class boundary”\n\nsvm_tune_spec &lt;- parsnip::svm_rbf(cost = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"classification\")\n\nThe hyperparameters are then tuned, in effect running the model in multiple subsets of the cross-validation to get a “best fit”.\n\nsvm_grid &lt;- tune::tune_grid(\n  workflows::add_model(thyroid_workflow, svm_tune_spec),\n  resamples = cv_folds, ## cv_loo,\n  grid = dials::grid_regular(cost(), levels = 20)\n)\n\nThe best fit is selected and fit to the overall training data set.\n\nsvm_highest_roc_auc &lt;- svm_grid |&gt;\n  tune::select_best(\"roc_auc\")\nfinal_svm &lt;- tune::finalize_workflow(\n  add_model(thyroid_workflow, svm_tune_spec),\n  svm_highest_roc_auc\n)"
  },
  {
    "objectID": "modelling.html#model-assessment",
    "href": "modelling.html#model-assessment",
    "title": "Modelling",
    "section": "Model Assessment",
    "text": "Model Assessment\nWhen considering the utility of prediction there are a number of metrics that can be used to determine how useful a model is. These focus on a number of terms which have very specific statistical meaning such as Sensitivity and Specificity.\n\n\n\n\nTest Positive\nTest Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nThere are a wealth of metrics based on combinations of these True/False Positive/Negative including precision, recall and so forth (refer to the above linked article for further details). The Tidymodels package for summarising model performance is yardstick and we will use that to summarise the different metrics of model performance and how predictive these are.\nConfusion matrices such as the above can be easily generated and even plotted using yardstick::confg_mat() function.\n\nPredictions\nWe could compare the predicted status with the true status in the training sample which was used to fit the data to the model, but what we are really interested in is its performance outside of this sample, i.e. in the test proportion that were held out of the model fitting process. Fortunately Tidymodels has packages that make this easy as the tune::last_fit() does this automatcally when given the final fitted model and the object that splits the data into two (in this case the split object we created right after simluating the data).\nWe have in fact already made predictions throughout the modelling process already when we produced the various confusion matrices but we shall repeat them for each of the fitted models so its easier to make direct comparisons.\n## LASSO\nlasso_confmat &lt;- tune::last_fit(object = final_lasso_kfold, split = split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::conf_mat(final_pathology, .pred_class)\nknitr::kable(lasso_confmat$table, caption = \"LASSO\")\n## Random Forest\nrf_confmat &lt;- tune::last_fit(final_rf, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::conf_mat(final_pathology, .pred_class)\nknitr::kable(rf_confmat$table, caption = \"Random Forest\")\n## XGBoost\nxgboost_confmat &lt;- tune::last_fit(final_xgboost, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::conf_mat(final_pathology, .pred_class)\nknitr::kable(xgboost_confmat$table, caption = \"XGBoost\")\n## SVM\nsvm_confmat &lt;- tune::last_fit(final_svm, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::conf_mat(final_pathology, .pred_class)\nknitr::kable(svm_confmat$table, caption = \"SVM\")\n\n\n\n\nLASSO\n\n\n\nBenign\nMalignant\n\n\n\n\nBenign\n0\n0\n\n\nMalignant\n45\n99\n\n\n\n\n\n\nRandom Forest\n\n\n\nBenign\nMalignant\n\n\n\n\nBenign\n11\n19\n\n\nMalignant\n34\n80\n\n\n\n\n\n\nXGBoost\n\n\n\nBenign\nMalignant\n\n\n\n\nBenign\n0\n0\n\n\nMalignant\n45\n99\n\n\n\n\n\n\nSVM\n\n\n\nBenign\nMalignant\n\n\n\n\nBenign\n8\n16\n\n\nMalignant\n37\n83\n\n\n\n\n\n\n\nConfusion matrices for fitted models\n\n\n\nConfusion matrices such as these can be used to calculate an array of statistics such as sensitivity, specificity, positive and negative predicitve value, false discovery rate and soforth. The Yardstick package has a convenience method for the result of yardstick::conf_mat() (a yardstick confusion matrix object). In the section below these are combined across models into a single table that we print out.\nlasso_confmat_summary &lt;- summary(lasso_confmat)\n\nWarning: While computing binary `precision()`, no predicted events were detected (i.e.\n`true_positive + false_positive = 0`).\nPrecision is undefined in this case, and `NA` will be returned.\nNote that 45 true event(s) actually occurred for the problematic event level,\nBenign\nWhile computing binary `precision()`, no predicted events were detected (i.e.\n`true_positive + false_positive = 0`).\nPrecision is undefined in this case, and `NA` will be returned.\nNote that 45 true event(s) actually occurred for the problematic event level,\nBenign\n\nlasso_confmat_summary$model &lt;- \"LASSO\"\nrf_confmat_summary &lt;- summary(rf_confmat)\nrf_confmat_summary$model &lt;- \"Random Forest\"\nxgboost_confmat_summary &lt;- summary(xgboost_confmat)\n\nWarning: While computing binary `precision()`, no predicted events were detected (i.e.\n`true_positive + false_positive = 0`).\nPrecision is undefined in this case, and `NA` will be returned.\nNote that 45 true event(s) actually occurred for the problematic event level,\nBenign\nWhile computing binary `precision()`, no predicted events were detected (i.e.\n`true_positive + false_positive = 0`).\nPrecision is undefined in this case, and `NA` will be returned.\nNote that 45 true event(s) actually occurred for the problematic event level,\nBenign\n\nxgboost_confmat_summary$model &lt;- \"XGBoost\"\nsvm_confmat_summary &lt;- summary(svm_confmat)\nsvm_confmat_summary$model &lt;- \"SVM\"\n\n## Bind these to each others by row to produce a data frame in \"long format\" which we tidy (remove the '.estimator')\n## column and reshape.\nmodel_summary &lt;- rbind(\n  svm_confmat_summary,\n  rf_confmat_summary,\n  xgboost_confmat_summary,\n  lasso_confmat_summary\n)\ndrop &lt;- c(\".estimator\")\nmodel_summary &lt;- model_summary[, !(names(model_summary) %in% drop)]\ncolnames(model_summary) &lt;- c(\"Metric\", \"estimate\", \"model\")\nmodel_summary &lt;- model_summary |&gt;\n  dplyr::mutate(Metric = case_when(\n    Metric == \"accuracy\" ~ \"Accuracy\",\n    Metric == \"kap\" ~ \"Kappa\",\n    Metric == \"sens\" ~ \"Sensitivity\",\n    Metric == \"spec\" ~ \"Specificity\",\n    Metric == \"ppv\" ~ \"Positive Predictive Value\",\n    Metric == \"npv\" ~ \"Negative Predictive Value\",\n    Metric == \"mcc\" ~ \"Matthews Correlation Coefficient\",\n    Metric == \"j_index\" ~ \"Youden's J-Statistic\",\n    Metric == \"bal_accuracy\" ~ \"Balanced Accuracy\",\n    Metric == \"detection_prevalence\" ~ \"Detection Prevelance\",\n    Metric == \"precision\" ~ \"Precision\",\n    Metric == \"recall\" ~ \"Recall\",\n    Metric == \"f_meas\" ~ \"F-Measure\"\n  ))\ntidyr::pivot_wider(model_summary, names_from = \"model\", values_from = \"estimate\") |&gt;\n  knitr::kable()\n\n\n\n\nSummary Statistics of Model Accuracy\n\n\n\n\n\n\n\n\n\nMetric\nSVM\nRandom Forest\nXGBoost\nLASSO\n\n\n\n\nAccuracy\n0.6319444\n0.6319444\n0.6875\n0.6875\n\n\nKappa\n0.0185185\n0.0577778\n0.0000\n0.0000\n\n\nSensitivity\n0.1777778\n0.2444444\n0.0000\n0.0000\n\n\nSpecificity\n0.8383838\n0.8080808\n1.0000\n1.0000\n\n\nPositive Predictive Value\n0.3333333\n0.3666667\nNaN\nNaN\n\n\nNegative Predictive Value\n0.6916667\n0.7017544\n0.6875\n0.6875\n\n\nMatthews Correlation Coefficient\n0.0201008\n0.0599486\nNA\nNA\n\n\nYouden’s J-Statistic\n0.0161616\n0.0525253\n0.0000\n0.0000\n\n\nBalanced Accuracy\n0.5080808\n0.5262626\n0.5000\n0.5000\n\n\nDetection Prevelance\n0.1666667\n0.2083333\n0.0000\n0.0000\n\n\nPrecision\n0.3333333\n0.3666667\nNA\nNA\n\n\nRecall\n0.1777778\n0.2444444\n0.0000\n0.0000\n\n\nF-Measure\n0.2318841\n0.2933333\nNA\nNA\n\n\n\n\n\n\n\n\nReceiver Operating Characteristics\nWe can use the yardstick package to facilitate visualising the [ReceThe Receiver Operating Characteristic (ROC) is a useful graphical tool for potting the false positive rate v’s the true positive rate (sensitivity). The roc_curve() function in yardstick makes plotting the ROC curvers relatively straight-forward.\n\n## ROC : LASSO\nroc_auc_lasso &lt;- tune::last_fit(final_lasso_kfold, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_auc(final_pathology, .pred_Malignant)\nroc_lasso &lt;- tune::last_fit(final_lasso_kfold, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_curve(truth = final_pathology, .pred_Malignant)\nroc_lasso$model &lt;- paste0(\"LASSO (\", roc_auc_lasso$.estimate, \")\")\n\n## ROC : Random Forest\nroc_auc_rf &lt;- tune::last_fit(final_rf, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_auc(final_pathology, .pred_Malignant)\nroc_rf &lt;- tune::last_fit(final_rf, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_curve(truth = final_pathology, .pred_Malignant)\nroc_rf$model &lt;- paste0(\"Random Forest (\", roc_auc_rf$.estimate, \")\")\n\n## ROC : Random XGBoost\nroc_auc_xgboost &lt;- tune::last_fit(final_xgboost, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_auc(final_pathology, .pred_Malignant)\nroc_xgboost &lt;- tune::last_fit(final_xgboost, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_curve(truth = final_pathology, .pred_Malignant)\nroc_xgboost$model &lt;- paste0(\"XGBoost (\", roc_auc_xgboost$.estimate, \")\")\n\n## ROC : SVM\nroc_auc_svm &lt;- tune::last_fit(final_svm, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_auc(final_pathology, .pred_Malignant)\nroc_svm &lt;- tune::last_fit(final_svm, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_curve(truth = final_pathology, .pred_Malignant)\nroc_svm$model &lt;- paste0(\"SVM (\", roc_auc_svm$.estimate, \")\")\n\n## Combine ROC curves across all three models and plot\nroc_all &lt;- rbind(roc_lasso, roc_rf, roc_xgboost, roc_svm)\nroc_all |&gt; ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  dark_theme_minimal()\n\n\n\n\nROC Curves"
  },
  {
    "objectID": "modelling.html#todo",
    "href": "modelling.html#todo",
    "title": "Modelling",
    "section": "TODO",
    "text": "TODO\nI still have some things I need to add to the above workflow…\n\nPredictive accuracy on the training data set and comparing to the test cohorts (if the predictive accuracy is worse in the later then the models are being over-fitted).\nCalculate ROC AUC for each model and add to legend of ROC curve plot (see yardstick::roc_auc()).\nAdd note that in the Sheffield data set the results of the LASSO may be used as a method of narrowing down which variables to use in subsequent models.\nAdd in Elastic Net regression as well as LASSO.\nIncrease the tune::metric_set() on which models are tuned perhaps? Most are roc_auc but can use ppv, precision, sens spec, recall, accuracy and others (see options under here).\nRemove the predictive accuracy from each subsection (at least in the output).\nSort column headings for the confusion matrices using #| tbl-subcap:       [] .\nTune the XGBoost parameters, currently have the following errors\n\n &gt; roc_auc_xgboost &lt;- tune::last_fit(final_xgboost, split) |&gt;\n  tune::collect_predictions() |&gt;\n  yardstick::roc_auc(final_pathology, .pred_Malignant)\n+ + Error:\n! 4 arguments have been tagged for tuning in these components: model_spec.\nPlease use one of the tuning functions (e.g. `tune_grid()`) to optimize them.\nRun `rlang::last_trace()` to see where the error occurred."
  },
  {
    "objectID": "modelling.html#notes",
    "href": "modelling.html#notes",
    "title": "Modelling",
    "section": "Notes",
    "text": "Notes\nIf you want to produce a plain R script of the code chunks in this document (or any Quarto document for that matter) you can do so using knirt::purl() function and sections which have #| purl: true will be included (all sections in this page have that added, none of the other Quarto documents have that as of writing). The following will take as input docs/modelling.qmd and make an R script of just the code cells in r/modelling.R.\n\nknitr::purl(\"docs/modelling.qmd\", output = \"r/modelling.R\")"
  },
  {
    "objectID": "modelling.html#references",
    "href": "modelling.html#references",
    "title": "Modelling",
    "section": "References",
    "text": "References\n\n\nSteyerberg, Ewout W., Marinus J. C. Eijkemans, Frank E. Harrell, and J. Dik F. Habbema. 2001. “Prognostic Modeling with Logistic Regression Analysis: In Search of a Sensible Strategy in Small Data Sets.” Med. Decis. Making 21 (1): 45–56. https://doi.org/10.1177/0272989X0102100106.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267–88. https://doi.org/10.2307/2346178."
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Literature",
    "section": "",
    "text": "Reviews of the existing literature around predictive modelling of Thyroid cancer are detailed below."
  },
  {
    "objectID": "literature.html#improving-the-diagnosis-of-thyroid-cancer-by-machine-learning-and-clinical-data",
    "href": "literature.html#improving-the-diagnosis-of-thyroid-cancer-by-machine-learning-and-clinical-data",
    "title": "Literature",
    "section": "Improving the diagnosis of thyroid cancer by machine learning and clinical data",
    "text": "Improving the diagnosis of thyroid cancer by machine learning and clinical data\nUsed a range of models (logistic regression, gradient boosting, linear discriminant analysis, support vector machine and random forest) to predict malignancy based on 18 predictor variables. Assessed via accuracy, precision, sensitivity and specificity and Area Under the Recevier Operating Characteristic (ROC).\n\nMethods\n10-fold cross validation (splitting data into ten subsets)\n\n\nPerformance\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nAUROC\nSensitivity\nSpecificity\nPrecision\n\n\n\n\nGBM\n0.7741\n0.8497\n0.8750\n0.5741\n0.8029\n\n\nLogistic\n0.7834\n0.8422\n0.8352\n0.6806\n0.8384\n\n\nLDA\n0.7790\n0.8394\n0.8452\n0.6477\n0.8263\n\n\nSVM (Radial)\n0.7688\n0.8237\n0.8435\n0.6206\n0.8149\n\n\nSVM (Linear)\n0.7661\n0.8200\n0.8322\n0.6349\n0.8186\n\n\nRandom Forest\n0.7931\n0.8541\n0.8629\n0.6547\n0.8321\n\n\n\nTable 2\n\n\n\nFigure 3\n\n\n\n\nVariable Importance\nInteresting approach using permutation prediction importance.\nSix most important variables are shown in table 6 and are all features of the nodules, no other biological features such as gender or age.\n\n\n\nFigure 4"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This work is being undertaken by Ovie Edafe as part of a PhD under the supervision of Karen Sisley and Sabapathy Balasubramanian at The University of Sheffield and in collaboration with Neil Shephard."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Thyroid Nodule App"
  },
  {
    "objectID": "links.html#thyroid-prediction-tools",
    "href": "links.html#thyroid-prediction-tools",
    "title": "Links",
    "section": "",
    "text": "Thyroid Nodule App"
  },
  {
    "objectID": "links.html#software",
    "href": "links.html#software",
    "title": "Links",
    "section": "Software",
    "text": "Software\n\nR\nRStudio\nQuarto\nLet’s Git started | Happy Git and GitHub for the useR\nR for Data Science (2e)\nTidy Modeling with R\nR for Non-Programmers: A Guide for Social Scientists"
  }
]